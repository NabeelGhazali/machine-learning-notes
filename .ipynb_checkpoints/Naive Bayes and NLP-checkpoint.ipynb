{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af514c39",
   "metadata": {},
   "source": [
    "### NAIVE BAYES ALGORITHM - PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a984a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayes theorem is a probability formula that leverages previously known probabilities to define probability of the related events occuring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943d9dcf",
   "metadata": {},
   "source": [
    "### NAIVE BAYES ALGORITHM - PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayes theoram to machien learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787cec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we assume that teh x features are mutually independant\n",
    "# when we extract features from a sentence which means words for a sentence\n",
    "# all teh words are not mutually independent\n",
    "# therefore it is a naive assumption that all words are mutualy independant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eb55f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# many variations of Naive Bayes:\n",
    "#     multinomial\n",
    "#     gausian\n",
    "#     complement\n",
    "#     bernoulli\n",
    "#     categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d102192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for NLP we will use multinoial naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9d2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer means breaking down a sentence or any text into words by performing preprocessing tasks like converting all words to lowercase, thus removing special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb801846",
   "metadata": {},
   "source": [
    "### EXTRACTING FEATURE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1395933d",
   "metadata": {},
   "source": [
    "##### THEORY AND INTUITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97bdc00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction from raw string text\n",
    "# from string to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3012291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main methods of feature extraction:\n",
    "# 1. count vectorization\n",
    "# 2. term frequency\n",
    "\n",
    "# count vectorization \n",
    "# counts every word in the document\n",
    "# stored in sparse matrix  to save space\n",
    "\n",
    "# DTM document term matrix\n",
    "\n",
    "# issues to consider:\n",
    "#     very common words\n",
    "#     words common to a particular set of documents (run in sports)\n",
    "#     stop words (a, the etc.)\n",
    "\n",
    "# we can address the issue of the document frequency by using a TF-IDF vectorization process\n",
    "# instead of filling the DTM with word frequency counts it calculates term frequency-inverse document frequency value for each word(TF-IDF)\n",
    "\n",
    "# term freq tf(t,d)\n",
    "# number of times that term t occurs in a document d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
